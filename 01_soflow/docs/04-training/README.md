# ğŸ“– Chapter 4: Training Objectives

<div align="center">

*Two complementary losses that make one-step generation possible*

</div>

---

## ğŸ¯ The Training Recipe

SoFlow uses **two losses** working together:

```
L_total = L_FM + Î» Â· L_cons
```

![Training Losses](./images/training-losses.svg)

---

## 1ï¸âƒ£ Flow Matching Loss (L_FM)

### Purpose
> *"Learn to predict clean data from noisy input"*

### Formula
```
L_FM = ğ”¼[â€–f_Î¸(x_t, t, 0) - xâ‚€â€–Â²]
```

### Intuition
```
x_t (noisy) â”€â”€â”€â”€â”€â”€â†’ f_Î¸(x_t, t, 0) â‰ˆ xâ‚€ (clean)
                           â†“
                    Compare with ground truth!
```

### Code
```python
def flow_matching_loss(model, x_0, x_1, t):
    # Create noisy sample
    x_t = (1 - t) * x_0 + t * x_1
    
    # Predict clean data (s=0)
    s = torch.zeros_like(t)
    x_0_pred = model(x_t, t, s)
    
    # MSE loss
    return F.mse_loss(x_0_pred, x_0)
```

---

## 2ï¸âƒ£ Consistency Loss (L_cons)

### Purpose
> *"Ensure predictions are self-consistent across the trajectory"*

### The Idea

If our solution function is correct:
```
Direct path:   f(x_t, t, s)
Indirect path: f(x_l, l, s)  where x_l = f(x_t, t, l)

Both should give the SAME result!
```

![Consistency Visual](./images/consistency-visual.svg)

### Formula
```
L_cons = ğ”¼[â€–f_Î¸(x_t, t, s) - sg[f_Î¸(x_l, l, s)]â€–Â²]
```

> âš ï¸ **sg** = stop gradient (target is fixed)

### Code
```python
def consistency_loss(model, x_0, x_1, t, l, s):
    x_t = (1 - t) * x_0 + t * x_1
    x_l = (1 - l) * x_0 + l * x_1
    
    # Direct prediction
    pred_direct = model(x_t, t, s)
    
    # Indirect prediction (stop gradient!)
    with torch.no_grad():
        pred_indirect = model(x_l, l, s)
    
    return F.mse_loss(pred_direct, pred_indirect)
```

---

## ğŸ›‘ Why Stop Gradient?

<table>
<tr>
<td width="50%">

### âŒ Without Stop Gradient
- Both predictions push toward each other
- Could collapse to trivial solution
- Unstable training

</td>
<td width="50%">

### âœ… With Stop Gradient
- Target is fixed
- Model learns to match it
- Stable, like EMA targets

</td>
</tr>
</table>

---

## ğŸ“Š The Consistency Loss Diagram

![Consistency Loss](./images/consistency-loss.svg)

> ğŸ’¡ This is Figure 2 from the original paper!

---

## ğŸ“ˆ Training Schedule

### The Problem
Early in training, the model is bad â†’ consistency targets are noisy.

### The Solution
Start easy, gradually increase difficulty:

![Schedule](./images/schedule.svg)

```python
# Schedule function
r = max(0.1, 1.0 - step / total_steps)
l = r * t  # l starts close to t, ends close to 0
```

| Training | `r` value | Difficulty |
|:--------:|:---------:|:----------:|
| Start | ~1.0 | Easy (l â‰ˆ t) |
| Middle | ~0.5 | Medium |
| End | ~0.1 | Hard (l â‰ˆ 0) |

---

## ğŸ”— Putting It Together

```python
def train_step(model, x_0, y, step, total_steps):
    x_1 = torch.randn_like(x_0)
    t = torch.rand(B) * 0.95 + 0.05
    
    # === Flow Matching Loss ===
    x_t = (1 - t) * x_0 + t * x_1
    s = torch.zeros(B)
    x_0_pred = model(x_t, t, s, y)
    loss_fm = F.mse_loss(x_0_pred, x_0)
    
    # === Consistency Loss ===
    r = max(0.1, 1.0 - step / total_steps)
    l = r * t
    x_l = (1 - l) * x_0 + l * x_1
    
    with torch.no_grad():
        x_0_target = model(x_l, l, s, y)
    
    loss_cons = F.mse_loss(x_0_pred, x_0_target)
    
    # === Combined ===
    return loss_fm + 0.1 * loss_cons
```

---

## ğŸ¤” Why Two Losses?

| Loss | Alone | Problem |
|:----:|:-----:|:-------:|
| **FM Only** | âœ… Learns denoising | âŒ No trajectory consistency |
| **Cons Only** | âœ… Self-consistent | âŒ No ground truth signal |
| **Combined** | âœ… Both! | âœ… Perfect! |

---

## ğŸ”‘ Key Takeaways

<table>
<tr>
<td>

### ğŸ“š Summary
- **L_FM**: Supervised denoising
- **L_cons**: Self-consistency
- **Schedule**: Easy â†’ Hard

</td>
<td>

### ğŸ’ª Benefits
- Stable training
- No JVP needed!
- One-step generation

</td>
</tr>
</table>

---

## ğŸ“š What's Next?

Want the mathematical guarantees?

<div align="center">

**[â† Chapter 3: Solution Function](../03-solution-function/README.md)** | **[Chapter 5: Proofs â†’](../05-proofs/README.md)**

</div>

---

<div align="center">

*Chapter 4 of 9 â€¢ [Back to Index](../README.md)*

</div>
