# ğŸ“– Chapter 5: Mathematical Proofs

<div align="center">

*Rigorous guarantees for why SoFlow works*

</div>

---

## ğŸ“ Overview

This chapter covers the **theoretical foundations** of SoFlow:

![Mathematical Framework](./images/math-framework.svg)

---

## ğŸ“œ Theorem 1: Solution Properties

### Statement

A valid solution function `f(x_t, t, s)` satisfies:

| Property | Formula | Meaning |
|:--------:|:-------:|:-------:|
| **Identity** | `f(x_t, t, t) = x_t` | No movement when s=t |
| **Composition** | `f(f(x_t,t,l), l, s) = f(x_t,t,s)` | Paths compose |
| **ODE** | `âˆ‚f/âˆ‚s = v(f, s)` | Derivative = velocity |

### Proof Sketch (Identity)

```
f(x_t, t, t) = x_t + âˆ«[tâ†’t] v(...) dÏ„
             = x_t + 0
             = x_t  âˆ
```

> ğŸ’¡ The integral over zero range is zero!

---

## ğŸ“œ Theorem 2: Residual Bound

### Definition

The **residual** measures how much our model deviates from the true solution:

```
R_Î¸(x_t, t, s) = "deviation from true solution"
```

### Bound

If training loss is bounded by `e_max`:

```
â€–R_Î¸â€– â‰¤ Î´ = e_max + H|s-t|r(k, K)
```

| Symbol | Meaning |
|:------:|:-------:|
| `e_max` | Max training error |
| `H` | Lipschitz constant |
| `r(k,K)` | Schedule function |

---

## ğŸ“œ Theorem 3: Global Error

### The Main Result

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â€–f - f_Î¸â€– â‰¤ |s - t| Â· Î´                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What This Means

For one-step generation (`t=1, s=0`):

```
Error â‰¤ 1 Â· Î´ = Î´
```

> ğŸ“‰ As training progresses, `Î´ â†’ 0`, so error â†’ 0!

---

## ğŸ“œ Theorem 4: ODE Error

### Statement

Training **implicitly minimizes** the ODE error:

```
â€–âˆ‚_s f_Î¸ - v(f_Î¸, s)â€– = O(âˆšÎ´)
```

### Interpretation

- We never explicitly compute ODE error
- But our losses minimize it automatically!
- This is why the solution function works

---

## ğŸš« Why No JVP?

### The JVP Problem

Other methods (Consistency, MeanFlow) need:

```python
# Jacobian-Vector Product (SLOW!)
jvp = âˆ‚v(x,t)/âˆ‚x Â· direction
```

**Problems:**
- ğŸ˜« Expensive backprop through model
- ğŸ˜« Poorly optimized in PyTorch
- ğŸ˜« High memory usage

### SoFlow's Approach

```python
# Just two forward passes (FAST!)
pred1 = model(x_t, t, s)
with torch.no_grad():
    pred2 = model(x_l, l, s)
loss = mse(pred1, pred2)
```

> ğŸš€ **2-3Ã— faster** than JVP methods!

---

## ğŸ“Š Theoretical Guarantees Summary

| Property | Guarantee |
|:--------:|:---------:|
| **Global Error** | `â‰¤ |s-t| Â· Î´` |
| **ODE Error** | `O(âˆšÎ´)` |
| **Convergence** | `Î´ â†’ 0` as training progresses |
| **Computation** | No JVP required! |

---

## ğŸ”‘ Key Takeaways

<table>
<tr>
<td>

### ğŸ“š The Math
- Solution function has 3 properties
- Error is bounded by residual
- ODE error minimized implicitly

</td>
<td>

### ğŸ’ª The Payoff
- Theoretical guarantees
- Convergence proof
- JVP-free = faster training

</td>
</tr>
</table>

---

## ğŸ“š What's Next?

How does Classifier-Free Guidance work in SoFlow?

<div align="center">

**[â† Chapter 4: Training](../04-training/README.md)** | **[Chapter 6: CFG â†’](../06-cfg/README.md)**

</div>

---

<div align="center">

*Chapter 5 of 9 â€¢ [Back to Index](../README.md)*

</div>
