# ğŸ“– Chapter 6: Classifier-Free Guidance

<div align="center">

*How SoFlow enables CFG without expensive JVP computation*

</div>

---

## ğŸ¨ What is CFG?

**Classifier-Free Guidance** improves generation quality by amplifying class-conditional signals:

![CFG Concept](./images/cfg-concept.svg)

### The Formula

```
á¹½ = v_uncond + w Â· (v_cond - v_uncond)
```

| Symbol | Meaning |
|:------:|:-------:|
| `v_cond` | Velocity with class info |
| `v_uncond` | Velocity without class |
| `w` | Guidance scale (1.5-7.0) |

> ğŸ’¡ Higher `w` = stronger class alignment, but less diversity

---

## ğŸ˜« The Problem with One-Step Models

### Standard Consistency Models

```python
# Consistency Model
image = model(noise)  # No velocity, no CFG!
```

âŒ **Problem**: No access to velocity â†’ Can't apply CFG naturally

### MeanFlow's Solution

```python
# MeanFlow - uses JVP
velocity = compute_jvp(model, x, t)  # ğŸ˜« Expensive!
```

âŒ **Problem**: JVP is slow and memory-hungry

---

## âœ¨ SoFlow's Solution

### Velocity Extraction

We can **extract velocity** from the solution function:

```python
def get_velocity(model, x_t, t, y, eps=1e-4):
    s = t - eps
    f_out = model(x_t, t, s, y)
    velocity = (f_out - x_t) / (-eps)
    return velocity
```

> ğŸ”‘ **Key Insight**: The derivative of solution gives velocity!

### CFG with Extracted Velocities

```python
def cfg_velocity(model, x_t, t, y, cfg_scale):
    # Get conditional velocity
    v_cond = get_velocity(model, x_t, t, y)
    
    # Get unconditional velocity
    v_uncond = get_velocity(model, x_t, t, y=None)
    
    # Apply CFG
    v_cfg = v_uncond + cfg_scale * (v_cond - v_uncond)
    return v_cfg
```

---

## ğŸ¯ CFG for One-Step Sampling

For final generation, apply CFG directly to outputs:

```python
def sample_with_cfg(model, noise, y, cfg_scale=2.0):
    t = torch.ones(B)   # Start time
    s = torch.zeros(B)  # Target time
    
    # Conditional prediction
    f_cond = model(noise, t, s, y)
    
    # Unconditional prediction
    f_uncond = model(noise, t, s, y=None)
    
    # CFG combination
    return f_uncond + cfg_scale * (f_cond - f_uncond)
```

---

## ğŸ“Š CFG Scale Effects

| Scale | Quality | Diversity | Class Match |
|:-----:|:-------:|:---------:|:-----------:|
| 1.0 | â­â­â­ | â­â­â­â­â­ | â­â­ |
| 1.5 | â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| 2.0 | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| 4.0 | â­â­â­â­ | â­â­ | â­â­â­â­â­ |
| 7.0 | â­â­â­ | â­ | â­â­â­â­â­ |

> ğŸ’¡ **Sweet spot**: CFG scale 2.0-4.0

---

## ğŸ†š Method Comparison

| Method | CFG? | How? | Cost |
|:------:|:----:|:----:|:----:|
| Consistency | âš ï¸ | Post-hoc | Medium |
| Distillation | âœ… | Teacher | High |
| MeanFlow | âœ… | JVP | **High** |
| **SoFlow** | âœ… | Velocity extraction | **Low!** |

---

## ğŸ”‘ Key Takeaways

<table>
<tr>
<td>

### ğŸ“š What We Learned
- CFG amplifies class signals
- Solution â†’ Velocity via derivative
- No JVP needed!

</td>
<td>

### ğŸ‰ Benefits
- Natural CFG integration
- Training-time CFG
- Test-time CFG
- Fast computation

</td>
</tr>
</table>

---

## ğŸ“š What's Next?

Let's look at the model architecture!

<div align="center">

**[â† Chapter 5: Proofs](../05-proofs/README.md)** | **[Chapter 7: Architecture â†’](../07-architecture/README.md)**

</div>

---

<div align="center">

*Chapter 6 of 9 â€¢ [Back to Index](../README.md)*

</div>
