# ğŸ§  Autoencoders: From Compression to Generation

<div align="center">

![Autoencoder Family](./images/autoencoder_family.svg)

*A comprehensive guide to understanding autoencoders and their variants*

[![Theory](https://img.shields.io/badge/Focus-Theory%20%2B%20Math-blueviolet?style=for-the-badge)](#)
[![Deep Learning](https://img.shields.io/badge/Domain-Deep%20Learning-blue?style=for-the-badge)](#)
[![Generative AI](https://img.shields.io/badge/Application-Generative%20AI-green?style=for-the-badge)](#)

</div>

---

## ğŸ“– What You'll Learn

This repository provides a **deep dive** into autoencoder architecturesâ€”from the foundational vanilla autoencoder to state-of-the-art hierarchical VAEs. Each section includes:

- ğŸ“ **Rigorous mathematical foundations**
- ğŸ¨ **Beautiful SVG visualizations**
- ğŸ’¡ **When and why to use each variant**
- ğŸ”¬ **Research references and papers**
- âœï¸ **Hands-on exercises**

---

## ğŸ—‚ï¸ Repository Structure

```
01_autoencoders/
â”‚
â”œâ”€â”€ ğŸ“ 01_vanilla_autoencoder/     # Foundation: encode-decode architecture
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ images/
â”‚
â”œâ”€â”€ ğŸ“ 02_sparse_autoencoder/      # Interpretable features via sparsity
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ images/
â”‚
â”œâ”€â”€ ğŸ“ 03_denoising_autoencoder/   # Robust features + diffusion connection
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ images/
â”‚
â”œâ”€â”€ ğŸ“ 04_variational_autoencoder/ # Probabilistic generation
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ ğŸ“ 01_elbo_and_kl/         # ELBO derivations
â”‚   â”œâ”€â”€ ğŸ“ 02_reparameterization_trick/
â”‚   â”œâ”€â”€ ğŸ“ 03_beta_vae/            # Disentanglement
â”‚   â””â”€â”€ ğŸ“ 04_hierarchical_vae/    # NVAE, VDVAE
â”‚
â””â”€â”€ ğŸ“ 05_autoencoder_evaluation/  # Metrics and evaluation
    â”œâ”€â”€ README.md
    â””â”€â”€ images/
```

---

## ğŸ¯ Quick Guide: Which Autoencoder Should You Use?

<table>
<tr>
<th>Your Goal</th>
<th>Best Choice</th>
<th>Why</th>
</tr>
<tr>
<td>ğŸ—œï¸ <b>Compression / Dimensionality Reduction</b></td>
<td><a href="./01_vanilla_autoencoder/">Vanilla AE</a></td>
<td>Simple, fast, effective for reconstruction</td>
</tr>
<tr>
<td>ğŸ” <b>Interpretable Features / LLM Analysis</b></td>
<td><a href="./02_sparse_autoencoder/">Sparse AE</a></td>
<td>Each neuron = distinct concept</td>
</tr>
<tr>
<td>ğŸ”Š <b>Noisy Data / Self-supervised Pretraining</b></td>
<td><a href="./03_denoising_autoencoder/">Denoising AE</a></td>
<td>Robust features, BERT-style training</td>
</tr>
<tr>
<td>ğŸ¨ <b>Generate New Samples</b></td>
<td><a href="./04_variational_autoencoder/">VAE</a></td>
<td>Probabilistic, smooth latent space</td>
</tr>
<tr>
<td>ğŸ›ï¸ <b>Control Specific Attributes</b></td>
<td><a href="./04_variational_autoencoder/03_beta_vae/">Î²-VAE</a></td>
<td>Disentangled = controllable</td>
</tr>
<tr>
<td>ğŸ† <b>State-of-the-art Generation</b></td>
<td><a href="./04_variational_autoencoder/04_hierarchical_vae/">Hierarchical VAE</a></td>
<td>NVAE/VDVAE quality</td>
</tr>
</table>

---

## ğŸ“š Learning Path

<div align="center">

![Learning Paths](./images/learning_paths.svg)

*Choose your path based on your experience level*

</div>

---

## ğŸ”¬ The Big Picture

### From Reconstruction to Generation

| Model | Can Generate? | Key Innovation |
|-------|--------------|----------------|
| **Vanilla AE** | âŒ | Bottleneck compression |
| **Sparse AE** | âŒ | Sparsity â†’ interpretability |
| **Denoising AE** | âŒ (but â†’ Diffusion) | Corruption â†’ robustness |
| **VAE** | âœ… | Probabilistic latent + ELBO |
| **Î²-VAE** | âœ… | Disentanglement via Î² |
| **HVAE** | âœ… | Multi-scale hierarchy |

### The Evolution

<div align="center">

![Autoencoder Evolution](./images/evolution_tree.svg)

*From compression to generation â€” the evolution of autoencoders*

</div>

---

## ğŸ§® Core Mathematics Preview

### The Autoencoder Objective

$$\mathcal{L}_{AE} = \mathbb{E}_{x \sim p_{data}} \left[ \| x - g_\theta(f_\phi(x)) \|^2 \right]$$

### The VAE Objective (ELBO)

$$\mathcal{L}_{VAE} = \underbrace{\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]}_{\text{Reconstruction}} - \underbrace{D_{KL}(q_\phi(z|x) \| p(z))}_{\text{Regularization}}$$

### The Reparameterization Trick

$$z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$

---

## ğŸ­ Industry Applications

| Industry | Application | Autoencoder Type |
|----------|-------------|------------------|
| ğŸ¥ **Healthcare** | Medical image denoising | Denoising AE |
| ğŸ’Š **Pharma** | Drug molecule generation | VAE |
| ğŸ® **Gaming** | Texture/level generation | VAE, HVAE |
| ğŸ” **Security** | Anomaly detection | Vanilla AE |
| ğŸ¤– **AI Research** | LLM interpretability | Sparse AE |
| ğŸ“¸ **Photography** | Image compression | Vanilla AE |

---

## ğŸ“– Key References

### Foundational Papers

1. **Rumelhart et al.** (1986) - *Learning representations by back-propagating errors*
2. **Kingma & Welling** (2014) - *Auto-Encoding Variational Bayes* (VAE)
3. **Higgins et al.** (2017) - *Î²-VAE: Learning Basic Visual Concepts*
4. **Vahdat & Kautz** (2020) - *NVAE: A Deep Hierarchical VAE*

### Recent Developments

5. **Cunningham et al.** (2023) - *Sparse Autoencoders for LLM Interpretability*
6. **Child** (2021) - *Very Deep VAEs Generalize Autoregressive Models*

---

## ğŸš€ Getting Started

### Prerequisites

- Linear algebra fundamentals
- Probability theory basics
- Neural network understanding
- Python + PyTorch (for exercises)

### Recommended Order

1. **Start here â†’** [Vanilla Autoencoder](./01_vanilla_autoencoder/)
2. **Then explore variants based on your goals**
3. **Deep dive into VAE theory** for generative modeling
4. **Practice with exercises** in each section

---

## ğŸ¤ Contributing

Found an error? Have a suggestion? Contributions are welcome!

- ğŸ“ Fix typos or clarify explanations
- ğŸ¨ Improve visualizations
- ğŸ“š Add references
- âœï¸ Contribute exercises

---

## ğŸ“œ License

This educational content is provided for learning purposes.

---

<div align="center">

**Happy Learning! ğŸ“**

*Understanding autoencoders is the gateway to modern generative AI*

</div>

